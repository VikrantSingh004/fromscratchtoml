{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fromscratchtoml.neural_network.models import Sequential\n",
    "from fromscratchtoml.neural_network.optimizers import StochasticGradientDescent, Adam\n",
    "from fromscratchtoml.neural_network.layers import Dense, Activation\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from fromscratchtoml.toolbox.random import Distribution\n",
    "from fromscratchtoml.toolbox.preprocess import to_onehot\n",
    "\n",
    "from fromscratchtoml.neural_network.layers import RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false \n",
    "import csv\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import itertools\n",
    "\n",
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "# Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "print (\"Reading CSV file...\")\n",
    "with open('mldata/reddit-comments-2015-08.csv', 'r') as f:\n",
    "#     reader = csv.reader(f, skipinitialspace=True)\n",
    "    reader = csv.DictReader(f)\n",
    "    # Split full comments into sentences\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x['body'].lower()) for x in reader])\n",
    "    # Append SENTENCE_START and SENTENCE_END\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print (\"Parsed %d sentences.\" % (len(sentences)))\n",
    "    \n",
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print (\"Found %d unique words tokens.\" % len(word_freq.items()))\n",
    "\n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "\n",
    "print (\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "print (\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
    "\n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "print (\"\\nExample sentence: '%s'\" % sentences[0])\n",
    "print (\"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# with open('company_data.pkl', 'wb') as output:\n",
    "#     pickle.dump(X_train, output, pickle.HIGHEST_PROTOCOL)\n",
    "#     pickle.dump(y_train, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# del X_train\n",
    "# del y_train\n",
    "\n",
    "with open('company_data.pkl', 'rb') as input1:\n",
    "    X_train = pickle.load(input1)\n",
    "    y_train = pickle.load(input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false \n",
    "import numpy as np\n",
    "# Create the training data\n",
    "X_train = np.array([np.array(([word_to_index[w] for w in sent[:-1]])) for sent in tokenized_sentences])\n",
    "y_train = np.array([np.array(([word_to_index[w] for w in sent[1:]])) for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    6, 3495,    7,  155,  796,   25,  222,    8,   32,   20,\n",
       "         202, 4954,  350,   91,    6,   66,  207,    5,    2]), 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0], len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(vocab_size=vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79170"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for x in X_train[0]:\n",
    "    t = np.zeros(8000)\n",
    "    t[x] = 1\n",
    "    temp.append(t)\n",
    "temp = np.array(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rnn.forward(temp)\n",
    "#rnn.forward(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 8000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential(verbose=1, vis_each_epoch=True)\n",
    "model1.add(RNN(vocab_size=vocabulary_size, seed=1, memory_window=4))\n",
    "model1.add(Activation('sigmoid'))\n",
    "sgd = StochasticGradientDescent(learning_rate=0.0005)\n",
    "model1.compile(optimizer=sgd, loss=\"cross_entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([y_train[0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(X, reverse=False):\n",
    "    temp = []\n",
    "    for word in X:\n",
    "        if reverse:\n",
    "            print(np.argmax(word), len(word))\n",
    "            temp.append(np.argmax(word))\n",
    "        else:\n",
    "            t = np.zeros(8000)\n",
    "            t[word] = 1\n",
    "            temp.append(t)\n",
    "    temp = np.array(temp)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 8000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_onehot(y_train[0]).shape\n",
    "# np.array([X_train[0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,) and (8000,100) not aligned: 1 (dim 0) != 8000 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-75b3f12d1ea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#model1.fit(np.array([temp]), np.array([y_train[0]]), epochs=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# model1.forwardpass(np.array(X_train[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/fromscratchtoml-0.0.2-py3.6.egg/fromscratchtoml/neural_network/models/sequential.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, epochs, batch_size)\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mbatch_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_batch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_batch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__update_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# if  self.verbose or epoch == epochs - 1:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/fromscratchtoml-0.0.2-py3.6.egg/fromscratchtoml/neural_network/models/sequential.py\u001b[0m in \u001b[0;36m__update_batch\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforwardpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_deriv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/fromscratchtoml-0.0.2-py3.6.egg/fromscratchtoml/neural_network/models/sequential.py\u001b[0m in \u001b[0;36mforwardpass\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/fromscratchtoml-0.0.2-py3.6.egg/fromscratchtoml/neural_network/layers/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, return_deriv)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_xh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_hh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0my_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_hy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,) and (8000,100) not aligned: 1 (dim 0) != 8000 (dim 0)"
     ]
    }
   ],
   "source": [
    "#model1.fit(np.array([temp]), np.array([y_train[0]]), epochs=1)\n",
    "model1.fit(X_train[:100], y_train[:100], epochs=1)\n",
    "# model1.forwardpass(np.array(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.predict(np.array([X_train[100]]), one_hot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[1], y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.layers[0].W_hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markroxor/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/fromscratchtoml-0.0.2-py3.6.egg/fromscratchtoml/toolbox/__init__.py:14: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/markroxor/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/markroxor/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/markroxor/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/markroxor/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/home/markroxor/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/home/markroxor/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/markroxor/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/markroxor/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/markroxor/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/markroxor/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/markroxor/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/markroxor/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/markroxor/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/markroxor/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/markroxor/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/markroxor/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/home/markroxor/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/markroxor/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/markroxor/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-7161701ac51f>\", line 2, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/home/markroxor/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 71, in <module>\n",
      "    from matplotlib.backends import pylab_setup\n",
      "  File \"/home/markroxor/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/matplotlib/backends/__init__.py\", line 16, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  matplotlib.use('agg')\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Adam'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7161701ac51f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlfromscratch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPlot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfromscratchtoml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneural_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfromscratchtoml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneural_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfromscratchtoml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneural_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Adam'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from mlfromscratch.utils import train_test_split, to_categorical, normalize, Plot\n",
    "from fromscratchtoml.neural_network.models import Sequential\n",
    "from fromscratchtoml.neural_network.optimizers import Adam\n",
    "from fromscratchtoml.neural_network.layers import Dense, Activation\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from fromscratchtoml.toolbox.random import Distribution\n",
    "from fromscratchtoml.toolbox.preprocess import to_onehot\n",
    "\n",
    "from fromscratchtoml.neural_network.layers import RNN\n",
    "\n",
    "def main():\n",
    "\n",
    "    optimizer = Adam()\n",
    "#     optimizer = StochasticGradientDescent()\n",
    "\n",
    "    def gen_mult_ser(nums):\n",
    "        \"\"\" Method which generates multiplication series \"\"\"\n",
    "        X = np.zeros([nums, 10, 61], dtype=float)\n",
    "        y = np.zeros([nums, 10, 61], dtype=float)\n",
    "        for i in range(nums):\n",
    "            start = np.random.randint(2, 7)\n",
    "            mult_ser = np.linspace(start, start*10, num=10, dtype=int)\n",
    "            X[i] = to_categorical(mult_ser, n_col=61)\n",
    "            y[i] = np.roll(X[i], -1, axis=0)\n",
    "        y[:, -1, 1] = 1 # Mark endpoint as 1\n",
    "        return X, y\n",
    "\n",
    "\n",
    "    def gen_num_seq(nums):\n",
    "        \"\"\" Method which generates sequence of numbers \"\"\"\n",
    "        X = np.zeros([nums, 10, 20], dtype=float)\n",
    "        y = np.zeros([nums, 10, 20], dtype=float)\n",
    "        for i in range(nums):\n",
    "            start = np.random.randint(0, 10)\n",
    "            num_seq = np.arange(start, start+10)\n",
    "            X[i] = to_categorical(num_seq, n_col=20)\n",
    "            y[i] = np.roll(X[i], -1, axis=0)\n",
    "        y[:, -1, 1] = 1 # Mark endpoint as 1\n",
    "        return X, y\n",
    "\n",
    "    X, y = gen_mult_ser(3000)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "\n",
    "    # Model definition\n",
    "#     clf = NeuralNetwork(optimizer=optimizer,\n",
    "#                         loss=CrossEntropy)\n",
    "    clf = Sequential()\n",
    "    clf.add(RNN(units=10, memory_window=5, vocab_size=61))\n",
    "    clf.add(Activation('softmax'))\n",
    "    clf.compile(optimizer=optimizer, loss='cross_entropy')\n",
    "#     clf.summary(\"RNN\")\n",
    "\n",
    "    # Print a problem instance and the correct solution\n",
    "    tmp_X = np.argmax(X_train[0], axis=1)\n",
    "    tmp_y = np.argmax(y_train[0], axis=1)\n",
    "    print (\"Number Series Problem:\")\n",
    "    print (\"X = [\" + \" \".join(tmp_X.astype(\"str\")) + \"]\")\n",
    "    print (\"y = [\" + \" \".join(tmp_y.astype(\"str\")) + \"]\")\n",
    "    print ()\n",
    "\n",
    "    train_err, _ = clf.fit(X_train, y_train, epochs=500, batch_size=512)\n",
    "\n",
    "    # Predict labels of the test data\n",
    "    y_pred = np.argmax(clf.predict(X_test), axis=2)\n",
    "    y_test = np.argmax(y_test, axis=2)\n",
    "\n",
    "    print ()\n",
    "    print (\"Results:\")\n",
    "    for i in range(5):\n",
    "        # Print a problem instance and the correct solution\n",
    "        tmp_X = np.argmax(X_test[i], axis=1)\n",
    "        tmp_y1 = y_test[i]\n",
    "        tmp_y2 = y_pred[i]\n",
    "        print (\"X      = [\" + \" \".join(tmp_X.astype(\"str\")) + \"]\")\n",
    "        print (\"y_true = [\" + \" \".join(tmp_y1.astype(\"str\")) + \"]\")\n",
    "        print (\"y_pred = [\" + \" \".join(tmp_y2.astype(\"str\")) + \"]\")\n",
    "        print ()\n",
    "    \n",
    "    accuracy = np.mean(accuracy_score(y_test, y_pred))\n",
    "    print (\"Accuracy:\", accuracy)\n",
    "\n",
    "    training = plt.plot(range(500), train_err, label=\"Training Error\")\n",
    "    plt.title(\"Error Plot\")\n",
    "    plt.ylabel('Training Error')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.show()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[█████████████████████████████████████   ] 92% \n",
      "epoch: 14/14  acc: 100.00  loss: 0.001 \n",
      "[████████████████████████████████████████] 100% "
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 1.38503523, -0.51962709]]), array([[-1.31788536,  1.49334281],\n",
       "        [-0.10027775, -1.39507145]], dtype=float128))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from fromscratchtoml.neural_network.models import Sequential\n",
    "from fromscratchtoml.neural_network.layers import Dense, Activation\n",
    "from fromscratchtoml.neural_network.optimizers import StochasticGradientDescent\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fromscratchtoml.toolbox.preprocess import to_onehot\n",
    "from fromscratchtoml.toolbox.random import Distribution\n",
    "\n",
    "\n",
    "\n",
    "X11 = Distribution.radial_binary(pts=300,\n",
    "               mean=[0, 0],\n",
    "               st=1,\n",
    "               ed=2, seed=20)\n",
    "X22 = Distribution.radial_binary(pts=300,\n",
    "               mean=[0, 0],\n",
    "               st=4,\n",
    "               ed=5, seed=10)\n",
    "\n",
    "Y11 = np.ones(X11.shape[0])\n",
    "Y22 = np.zeros(X11.shape[0])\n",
    "\n",
    "X = np.vstack((X11, X22))\n",
    "y = np.hstack((Y11, Y22))\n",
    "\n",
    "y = to_onehot(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=50, random_state=42)\n",
    "        \n",
    "        \n",
    "        \n",
    "model = Sequential(verbose=0)\n",
    "\n",
    "model.add(Dense(10, input_dim=2, seed=1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(2, seed=7))\n",
    "model.add(Activation('tanh'))\n",
    "\n",
    "model.add(Dense(2, seed=2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(2, seed=3))\n",
    "model.add(Activation('leaky_relu'))\n",
    "\n",
    "model.add(Dense(2, seed=4))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "model.add(Dense(2, seed=6))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "sgd = StochasticGradientDescent(learning_rate=0.05)\n",
    "model.compile(optimizer=sgd, loss=\"mean_squared_error\")\n",
    "\n",
    "model.fit(X_train, y_train, epochs=14, batch_size=4)\n",
    "\n",
    "expected_biases = np.array([[0.08650937, 1.00013189]], dtype=np.float128)\n",
    "# self.assertTrue(np.allclose(expected_biases, model.layers[-1].biases))\n",
    "\n",
    "expected_weights = np.array([[-0.49908263, -0.17316507], [-0.42623203, 0.48448988]], dtype=np.float128)\n",
    "# self.assertTrue(np.allclose(expected_weights, model.layers[-1].weights))\n",
    "model.accuracy(X_test, y_test)\n",
    "model.layers[-2].biases, model.layers[-2].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| RNN |\n",
      "+-----+\n",
      "Input Shape: (10, 61)\n",
      "+----------------------+------------+--------------+\n",
      "| Layer Type           | Parameters | Output Shape |\n",
      "+----------------------+------------+--------------+\n",
      "| RNN                  | 1320       | (10, 61)     |\n",
      "| Activation (Softmax) | 0          | (10, 61)     |\n",
      "+----------------------+------------+--------------+\n",
      "Total Parameters: 1320\n",
      "\n",
      "Number Series Problem:\n",
      "X = [4 8 12 16 20 24 28 32 36 40]\n",
      "y = [8 12 16 20 24 28 32 36 40 1]\n",
      "\n",
      "loss 0.09048720433257987\n",
      "(512, 61)\n",
      "(10, 61) no\n",
      "loss 0.09044689560591289\n",
      "(512, 61)\n",
      "(10, 61) no\n",
      "loss 0.0903972973036759\n",
      "(512, 61)\n",
      "(10, 61) no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% [                                               ] ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.09033328370452352\n",
      "(264, 61)\n",
      "(10, 61) no\n",
      "loss 0.090263718248511\n",
      "(512, 61)\n",
      "(10, 61) no\n",
      "loss 0.09019414932633613\n",
      "(512, 61)\n",
      "(10, 61) no\n",
      "loss 0.0901246764183648\n",
      "(512, 61)\n",
      "(10, 61) no\n",
      "loss 0.09004394854217194\n",
      "(264, 61)\n",
      "(10, 61) no\n",
      "loss 0.08996327567058172\n",
      "(512, 61)\n",
      "(10, 61) no\n",
      "loss 0.08988666647311848\n",
      "(512, 61)\n",
      "(10, 61) no\n",
      "loss 0.08981176237866573\n",
      "(512, 61)\n",
      "(10, 61) no\n",
      "loss 0.08972343768781156\n",
      "(264, 61)\n",
      "(10, 61) no\n",
      "loss 0.08963858109114234\n",
      "(512, 61)\n",
      "(10, 61) no\n",
      "loss 0.08955961835702578\n",
      "(512, 61)\n",
      "(10, 61) no\n",
      "loss 0.08948240394023542\n",
      "(512, 61)\n",
      "(10, 61) no\n",
      "loss 0.08938787091294614\n",
      "(264, 61)\n",
      "(10, 61) no\n",
      "loss 0.08929957370377885\n",
      "(512, 61)\n",
      "(10, 61) no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% [                                                ] ETA:  0:02:15\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.089218635904666\n",
      "(512, 61)\n",
      "(10, 61) no\n",
      "loss 0.0891390124280808\n",
      "(512, 61)\n",
      "(10, 61) no\n",
      "loss 0.08903737996073179\n",
      "(264, 61)\n",
      "(10, 61) no\n",
      "loss 0.0889445749193523\n",
      "(512, 61)\n",
      "(10, 61) no\n",
      "loss 0.08886077396715944\n",
      "(512, 61)\n",
      "(10, 61) no\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-16347a44f9dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-16347a44f9dd>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mtrain_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# Predict labels of the test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/mlfromscratch-0.0.4-py3.6.egg/mlfromscratch/deep_learning/neural_network.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, n_epochs, batch_size)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mbatch_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0mbatch_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/mlfromscratch-0.0.4-py3.6.egg/mlfromscratch/deep_learning/neural_network.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# print(\"av loss\", self.loss_function.loss(y, y_pred))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/mlfromscratch-0.0.4-py3.6.egg/mlfromscratch/deep_learning/neural_network.py\u001b[0m in \u001b[0;36m_backward_pass\u001b[0;34m(self, loss_grad)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;34m\"\"\" Propagate the gradient 'backwards' and update the weights in each layer \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mloss_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Model Summary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/mlfromscratch-0.0.4-py3.6.egg/mlfromscratch/deep_learning/layers.py\u001b[0m in \u001b[0;36mbackward_pass\u001b[0;34m(self, accum_grad)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mgrad_wrt_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccum_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;31m# Gradient w.r.t the layer input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0maccum_grad_next\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_wrt_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;31m# Update gradient w.r.t W and U by backprop. from time step t for at most\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;31m# self.bptt_trunc number of time steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from mlfromscratch.deep_learning import NeuralNetwork\n",
    "from mlfromscratch.utils import train_test_split, to_categorical, normalize, Plot\n",
    "from mlfromscratch.utils import get_random_subsets, shuffle_data, accuracy_score\n",
    "from mlfromscratch.deep_learning.optimizers import StochasticGradientDescent, Adam, RMSprop, Adagrad, Adadelta\n",
    "from mlfromscratch.deep_learning.loss_functions import CrossEntropy\n",
    "from mlfromscratch.utils.misc import bar_widgets\n",
    "from mlfromscratch.deep_learning.layers import RNN, Activation\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    optimizer = Adam()\n",
    "#     optimizer = StochasticGradientDescent()\n",
    "\n",
    "    def gen_mult_ser(nums):\n",
    "        \"\"\" Method which generates multiplication series \"\"\"\n",
    "        X = np.zeros([nums, 10, 61], dtype=float)\n",
    "        y = np.zeros([nums, 10, 61], dtype=float)\n",
    "        for i in range(nums):\n",
    "            start = np.random.randint(2, 7)\n",
    "            mult_ser = np.linspace(start, start*10, num=10, dtype=int)\n",
    "            X[i] = to_categorical(mult_ser, n_col=61)\n",
    "            y[i] = np.roll(X[i], -1, axis=0)\n",
    "        y[:, -1, 1] = 1 # Mark endpoint as 1\n",
    "        return X, y\n",
    "\n",
    "\n",
    "    def gen_num_seq(nums):\n",
    "        \"\"\" Method which generates sequence of numbers \"\"\"\n",
    "        X = np.zeros([nums, 10, 20], dtype=float)\n",
    "        y = np.zeros([nums, 10, 20], dtype=float)\n",
    "        for i in range(nums):\n",
    "            start = np.random.randint(0, 10)\n",
    "            num_seq = np.arange(start, start+10)\n",
    "            X[i] = to_categorical(num_seq, n_col=20)\n",
    "            y[i] = np.roll(X[i], -1, axis=0)\n",
    "        y[:, -1, 1] = 1 # Mark endpoint as 1\n",
    "        return X, y\n",
    "\n",
    "    X, y = gen_mult_ser(3000)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "\n",
    "    # Model definition\n",
    "    clf = NeuralNetwork(optimizer=optimizer,\n",
    "                        loss=CrossEntropy)\n",
    "    clf.add(RNN(10, activation=\"tanh\", bptt_trunc=5, input_shape=(10, 61)))\n",
    "    clf.add(Activation('softmax'))\n",
    "    clf.summary(\"RNN\")\n",
    "\n",
    "    # Print a problem instance and the correct solution\n",
    "    tmp_X = np.argmax(X_train[0], axis=1)\n",
    "    tmp_y = np.argmax(y_train[0], axis=1)\n",
    "    print (\"Number Series Problem:\")\n",
    "    print (\"X = [\" + \" \".join(tmp_X.astype(\"str\")) + \"]\")\n",
    "    print (\"y = [\" + \" \".join(tmp_y.astype(\"str\")) + \"]\")\n",
    "    print ()\n",
    "\n",
    "    train_err, _ = clf.fit(X_train, y_train, n_epochs=500, batch_size=512)\n",
    "\n",
    "    # Predict labels of the test data\n",
    "    y_pred = np.argmax(clf.predict(X_test), axis=2)\n",
    "    y_test = np.argmax(y_test, axis=2)\n",
    "\n",
    "    print ()\n",
    "    print (\"Results:\")\n",
    "    for i in range(5):\n",
    "        # Print a problem instance and the correct solution\n",
    "        tmp_X = np.argmax(X_test[i], axis=1)\n",
    "        tmp_y1 = y_test[i]\n",
    "        tmp_y2 = y_pred[i]\n",
    "        print (\"X      = [\" + \" \".join(tmp_X.astype(\"str\")) + \"]\")\n",
    "        print (\"y_true = [\" + \" \".join(tmp_y1.astype(\"str\")) + \"]\")\n",
    "        print (\"y_pred = [\" + \" \".join(tmp_y2.astype(\"str\")) + \"]\")\n",
    "        print ()\n",
    "    \n",
    "    accuracy = np.mean(accuracy_score(y_test, y_pred))\n",
    "    print (\"Accuracy:\", accuracy)\n",
    "\n",
    "    training = plt.plot(range(500), train_err, label=\"Training Error\")\n",
    "    plt.title(\"Error Plot\")\n",
    "    plt.ylabel('Training Error')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.show()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fs2ml",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
